{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SpellCorrectionPreProcess.ipynp",
      "provenance": [],
      "collapsed_sections": [
        "6_n1AE3-ctZg"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMimHhJWhdSh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cb301b0-3a5d-416b-cff8-b435a2572360"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5350gv_jhzwZ",
        "outputId": "c6c46963-b703-4818-c229-7167b2074583"
      },
      "source": [
        "cd gdrive/My\\ Drive/Information\\ retrieval/Project\\ 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/Information retrieval/Project 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exeaS2nyiF9g"
      },
      "source": [
        "# All Articles\n",
        "# !wget \"https://dumps.wikimedia.org/fawiki/20201120/fawiki-20201120-pages-articles-multistream.xml.bz2\"\n",
        "# !bzip2 -d fawiki-20201120-pages-articles-multistream.xml.bz2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R0kK5jT3nM5"
      },
      "source": [
        "# Abstract\n",
        "# !wget \"https://dumps.wikimedia.org/fawiki/20201101/fawiki-20201101-abstract.xml.gz\"\n",
        "# !gunzip fawiki-20201101-abstract.xml.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhK374MxiJFj",
        "outputId": "465e5770-9d69-4523-e8ae-bcdd713e2d10"
      },
      "source": [
        "!ls -lh"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 6.7G\n",
            "-rw------- 1 root root 1.4K Dec  8 16:06  charsSet.txt\n",
            "-rw------- 1 root root  459 Dec  8 15:10  chars.txt\n",
            "-rw------- 1 root root 280K Dec 16 08:02  DesignSeq2SeqModel.ipynb\n",
            "-rw------- 1 root root 709M Nov  4 01:16  fawiki-20201101-abstract.xml\n",
            "-rw------- 1 root root 5.8G Nov 21 16:25  fawiki-20201120-pages-articles-multistream.xml\n",
            "drwx------ 2 root root 4.0K Dec 16 07:10  savedmodels\n",
            "-rw------- 1 root root  43K Dec 25 20:36  SpellCorrectionPreProcess.ipynp\n",
            "drwx------ 2 root root 4.0K Dec 16 07:28  training_checkpoints\n",
            "-rw------- 1 root root 4.8M Dec 12 14:04 'VocabularyAbstract_v1.0-36char-afterTokenize&RemoveUnderlineSplitBy|.txt'\n",
            "-rw------- 1 root root 4.8M Dec 21 15:37 'VocabularyAbstract_v1.0-36char-Tokenize&RemoveUnderline2Words3WordsSplitByN-ReadyToBuildDataSet.txt'\n",
            "-rw------- 1 root root  22M Dec  8 19:23  Vocabulary.txt\n",
            "-rw------- 1 root root  23M Dec  9 09:17  Vocabulary_v1.2.txt\n",
            "-rw------- 1 root root  19M Dec  9 12:20  Vocabulary_v2.1.txt\n",
            "-rw------- 1 root root  22M Dec  9 14:53 'Vocabulary_v2.3-36char-afterTokenize&RemoveUnderlineSplitBy|.txt'\n",
            "-rw------- 1 root root  24M Dec  9 17:28 'Vocabulary_v3.0-36char-Tokenize&RemoveUnderline2Words3WordsSplitBy|-ReadyToBuildDataSet.txt'\n",
            "-rw------- 1 root root  22M Dec 24 15:44 'Vocabulary_v4.0-36char-afterTokenize&RemoveUnderlineSplitByN.txt'\n",
            "-rw------- 1 root root  24M Dec 12 19:16 'Vocabulary_v4.0-36char-Tokenize&RemoveUnderline2Words3WordsSplitByN-ReadyToBuildDataSet.txt'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_mbwPp3c5dn6"
      },
      "source": [
        "!head -n30 fawiki-20201101-abstract.xml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9lu-A0ULmyo_"
      },
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvIMLxlCi5_o"
      },
      "source": [
        "from lxml import etree\n",
        "\n",
        "class Wikipedia:\n",
        "    def __init__(self, fh, tag):\n",
        "        \"\"\"\n",
        "        Initialize 'iterparse' to only generate 'end' events on tag '<entity>'\n",
        "\n",
        "        :param fh: File Handle from the XML File to parse\n",
        "        :param tag: The tag to process\n",
        "        \"\"\"\n",
        "        # Prepend the default Namespace {*} to get anything.\n",
        "        self.context = etree.iterparse(fh, events=(\"end\",), tag=['{*}' + tag])\n",
        "\n",
        "    def _parse(self):\n",
        "        \"\"\"\n",
        "        Parse the XML File for all '<tag>...</tag>' Elements\n",
        "        Clear/Delete the Element Tree after processing\n",
        "\n",
        "        :return: Yield the current 'Event, Element Tree'\n",
        "        \"\"\"\n",
        "        for event, elem in self.context:\n",
        "            yield event, elem\n",
        "\n",
        "            elem.clear()\n",
        "            while elem.getprevious() is not None:\n",
        "                del elem.getparent()[0]\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"\n",
        "        Iterate all '<tag>...</tag>' Element Trees yielded from self._parse()\n",
        "\n",
        "        :return: Dict var 'entity' {tag1, value, tag2, value, ... ,tagn, value}}\n",
        "        \"\"\"\n",
        "        for event, elem in self._parse():\n",
        "            entity = {}\n",
        "\n",
        "            # Assign the 'elem.namespace' to the 'xpath'\n",
        "            entity['revision'] = elem.xpath('./xmlns:revision/xmlns:text/text( )', \n",
        "                                   namespaces={'xmlns':etree.QName(elem).namespace})\n",
        "\n",
        "            yield entity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGDGsYQ680fV"
      },
      "source": [
        "### Normalize and save and tokenize\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xxpsCDjxmGmI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66089608-04bd-40d5-b517-bfc623b0bae0"
      },
      "source": [
        "p = 0\n",
        "with open('fawiki-20201120-pages-articles-multistream.xml', 'rb') as in_xml:\n",
        "    chars = []\n",
        "    for record in Wikipedia(in_xml, tag='page'):\n",
        "        try:\n",
        "            chars.extend(list(set(list(str(record['revision'][0])))))\n",
        "        except Exception as ex:\n",
        "            pass\n",
        "        # break\n",
        "        if p%100000==0:\n",
        "            chars = list(set(chars))\n",
        "            print(p//100000, 'Len', len(chars), end=' ')\n",
        "        p += 1\n",
        "print(p)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 Len 108 1 Len 5736 2 Len 6817 3 Len 7513 4 Len 7744 5 Len 8184 6 Len 8825 7 Len 8964 8 Len 9142 9 Len 9191 10 Len 9245 11 Len 9390 12 Len 9439 13 Len 9554 14 Len 9573 15 Len 9625 16 Len 9893 17 Len 10031 18 Len 10135 19 Len 10726 20 Len 10808 21 Len 10882 22 Len 11103 23 Len 11300 24 Len 11476 25 Len 11680 26 Len 12188 27 Len 12309 28 Len 12339 29 Len 12786 30 Len 13082 31 Len 13207 32 Len 13409 33 Len 13749 3358604\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U054ab427mSn",
        "outputId": "795ae245-d85c-4c22-d3fb-14a9c6eee4b2"
      },
      "source": [
        "# Abstract\n",
        "import xml.etree.ElementTree as ET\n",
        "from time import time\n",
        "root = ET.parse('fawiki-20201101-abstract.xml').getroot()\n",
        "chars = []\n",
        "for i, item in enumerate(root.findall('./doc')):\n",
        "    if i%100000==0:\n",
        "        print(i//100000, end=' ')\n",
        "    if len(item)>2: \n",
        "        chars.extend(list(set(list(str(item[2].text))))) # item[2] is document body\n",
        "print(\"All Done.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1 2 3 4 5 6 7 All Done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LrifqR3P85k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-Ka8jF7mIhI",
        "outputId": "377f77f0-8267-4334-c7c2-28e49f40d66d"
      },
      "source": [
        "whitelist = list('\\u200cÿ¢-ŸÄÿ∂ÿµÿ´ŸÇŸÅÿ∫ÿπŸáÿÆÿ≠ÿ¨⁄Ü⁄Ø⁄©ŸÖŸÜÿ™ÿßŸÑÿ®€åÿ≥ÿ¥ÿ∏ÿ∑ÿ≤ÿ±ÿ∞ÿØŸæŸà⁄ò\\u200d Ÿì')\n",
        "stopchars = []\n",
        "for c in chars:\n",
        "    if not (c in whitelist):\n",
        "        stopchars.append(c)\n",
        "stopchars = list(set(stopchars))\n",
        "len(stopchars)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "13967"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHbsZvikwjy8"
      },
      "source": [
        "class Normalizer:\n",
        "    def __init__(self):\n",
        "        import re\n",
        "        self.whitelist = list('\\u200cÿ¢-ŸÄÿ∂ÿµÿ´ŸÇŸÅÿ∫ÿπŸáÿÆÿ≠ÿ¨⁄Ü⁄Ø⁄©ŸÖŸÜÿ™ÿßŸÑÿ®€åÿ≥ÿ¥ÿ∏ÿ∑ÿ≤ÿ±ÿ∞ÿØŸæŸà⁄ò\\u200d')\n",
        "        maketrans = lambda A, B: dict((ord(a), b) for a, b in zip(A, B))\n",
        "        compile_patterns = lambda patterns: [(re.compile(pattern), repl) for pattern, repl in patterns]\n",
        "        translation_src = ' ŸâŸÉŸäÿ¶‚Äú‚Äùÿ©ÿ£ÿ•ÿ§€Ñ'   # str(''.join(chars))\n",
        "        translation_dst = ' €å⁄©€å€å\"\"ŸáÿßÿßŸàŸà'   # str(''.join(newchars))\n",
        "        translation_src += '?!'\n",
        "        translation_dst += 'ÿü!'\n",
        "        self.translations = maketrans(translation_src, translation_dst)\n",
        "        \n",
        "        self.character_refinement_patterns = []\n",
        "        self.character_refinement_patterns.extend([\n",
        "            # (r'[ŸÄ\\r^A-Za-z0-9€∞-€π\\<\\>\\-\\=\\+\\\\\\/\\{\\}\\|\\[\\]:;\"&%Ÿ™#$\\*\\'_]', ''),  # remove keshide, English words, numbers, carriage returns\n",
        "            # ('\"([^\\n\"]+)\"', r'\\1'),  # replace quotation with gyoome\n",
        "            ('[\\u064B\\u064C\\u064D\\u064E\\u064F\\u0650\\u0651\\u0652\\u0654\\u0621]', ''),  # remove FATHATAN, DAMMATAN, KASRATAN, FATHA, DAMMA, KASRA, SHADDA, SUKUN\n",
        "            (r'[\\s \\=\\|\\(\\)\\[\\]\\{\\}\\:\\+\\?\\ÿü\\!\\!\\.\\,\\;\\'\\\"\\@\\#\\$\\%\\^\\&\\*\\`\\~\\<\\>\\/\\\\\\Ÿ¨\\Ÿ´\\Ô∑º\\Ÿ™\\√ó\\ÿå\\*\\=\\+\\√∑\\\\\\\\\\:€±€≤€≥€¥€µ€∂€∑€∏€π€∞ Ÿ†Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ©\\¬´\\¬ª]+', ' '),  #\n",
        "            (r'\\n+', '\\n'),  # remove extra newlines\n",
        "            (r'\\n.\\n.+', '\\n'),  # remove extra newlines\n",
        "            (r'\\n+', '\\n'),  # remove extra newlines\n",
        "        ])\n",
        "        self.character_refinement_patterns = compile_patterns(self.character_refinement_patterns)\n",
        "    def normalizeContent(self, text):\n",
        "        text = text.translate(self.translations)\n",
        "        for pattern, repl in self.character_refinement_patterns:\n",
        "            text = pattern.sub(repl, text)\n",
        "        return text\n",
        "    def normalizeTokens(self, token, pat):  # if token is acceptable (persian and lengh>1) return Normalized token, else return None\n",
        "        # print(\"NormalizingTokens Started...\")\n",
        "        token = re.sub(pat, '', str(token))\n",
        "        c = 1\n",
        "        out = False\n",
        "        if len(token)>1:\n",
        "            for i in range(len(token)-1):\n",
        "                if token[i] == token[i+1]:\n",
        "                    c+=1\n",
        "                    if c>2:\n",
        "                        out = True\n",
        "                        break\n",
        "                else :\n",
        "                    c = 1\n",
        "            if len(token)>1 and self.ispersian(token) and not out:\n",
        "                return token\n",
        "        return None\n",
        "\n",
        "    def tokenize(self, content):\n",
        "        paterns = '[?ÿü\\., !!ÿå¬ª¬´\\(\\)\\[\\]\\-\\s\\n::A-Za-z%\\\\\\#\\$\\&\\^\\*\\+\\=\\/\\<\\>\\@\\{\\}\\~\\'\\\"0123456789€∞€±€≤€≥€¥€µ€∂€∑€∏€πŸ†Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ© \\s/\\|]+'\n",
        "        content = re.split(paterns, content)\n",
        "        # print(content)\n",
        "        return list(set(content))\n",
        "\n",
        "    def getFreshTokens(self, article, pat):\n",
        "        newArt = []\n",
        "        for token in article:\n",
        "            token = self.normalizeTokens(token, pat)\n",
        "            if token != None:\n",
        "                newArt.append(token)\n",
        "        return newArt\n",
        "\n",
        "    def ispersian(self, token):\n",
        "        token = list(token)\n",
        "        i = 0\n",
        "        for c in token:\n",
        "            if c in whitelist:\n",
        "                i += 1\n",
        "        if i/len(token) > 0.7:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def getStopChars(self, articles):\n",
        "        print(\"Finding Char Set...\")\n",
        "        chars = []\n",
        "        for i, token in enumerate(l):\n",
        "            tchars = list(token)\n",
        "            chars.extend(tchars)\n",
        "            if i%200000==0:\n",
        "                chars = list(set(chars))\n",
        "                print(len(chars), end=\" \")\n",
        "        chars = list(set(chars))\n",
        "        print(len(chars), end=\" \")\n",
        "        stopchar = []\n",
        "        for c in chars:\n",
        "            if not (c in whitelist):\n",
        "                stopchar.append(c)\n",
        "        return stopchar\n",
        "\n",
        "norm = Normalizer()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXy97EktNdCU"
      },
      "source": [
        "stopchars = \"\".join(stopchars)\n",
        "stopchars = re.sub(r'[\\s \\=\\|\\(\\)\\[\\]\\{\\}\\:\\+\\?\\ÿü\\!\\!\\.\\,\\;\\'\\\"\\@\\#\\$\\%\\^\\&\\*\\`\\~\\<\\>\\/\\\\\\Ÿ¨\\Ÿ´\\Ô∑º\\Ÿ™\\√ó\\ÿå\\*\\=\\+\\√∑\\\\\\\\\\:€±€≤€≥€¥€µ€∂€∑€∏€π€∞ Ÿ†Ÿ°Ÿ¢Ÿ£Ÿ§Ÿ•Ÿ¶ŸßŸ®Ÿ©\\¬´\\¬ª_]+', '', stopchars)\n",
        "pat = r\"[\" + stopchars + ' Ÿç Ÿå Ÿí Ÿã Ÿè Ÿê Ÿé Ÿë \\:\\\\¬´\\¬ª\\ Ÿîÿ° \\ Ÿ¨\\ÿçŸ´ \\ Ÿ∞ Ÿì              \\u202e\\u202c\\u200e \\u202d\\u0600\\u200f\\u0602 ŸÄ\\ŸÄ' + \"]+\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OsBJ12NGvbuT",
        "outputId": "bbcf6594-b95f-4ffb-d89f-c250b15a533d"
      },
      "source": [
        "pat.find('_')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "toUwp0mhMGu0",
        "outputId": "7b814401-3ba0-47f7-8448-f21f42bce56b"
      },
      "source": [
        "re.sub(pat, '', 'Ôºë«†Èô©ÈóªË™øÔ∫∑ËûÇsadfÿ¥ÿ≥€åÿ®ÿ¥€åÿ®üêçÿ≥_ÿ®ÿ≥ÿ¥ŸÄŸÄŸÄŸÄ€åÿ® ÁπïÊµÅÁÑºÁµ°ÁâßÁ∂≠')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'ÿ¥ÿ≥€åÿ®ÿ¥€åÿ®ÿ≥_ÿ®ÿ≥ÿ¥€åÿ®'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJX-NZ6ko8Qo",
        "outputId": "c1ee67b2-cbe2-4ba2-d231-233909016390"
      },
      "source": [
        "norm.getFreshTokens(norm.tokenize(norm.normalizeContent(\"Ôºë«†Èô©Èóªÿ¥€åÿ®Ë™øÿ¥ÿ≥€åÿ®Ô∫∑ËûÇÿ¥ÿ≥€åÿ®ÿ¥€åÿ®üêçÿ≥ÿ®ÿ≥ÿ¥€åÿ® ÁπïÊµÅÁÑºÁµ°ÁâßÁ∂≠\")), pat)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['ÿ¥€åÿ®ÿ¥ÿ≥€åÿ®ÿ¥ÿ≥€åÿ®ÿ¥€åÿ®ÿ≥ÿ®ÿ≥ÿ¥€åÿ®']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tU90U3pfu8p4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4324fcb4-a4e3-4c7e-dbf9-65dad31059f2"
      },
      "source": [
        "# Articles\n",
        "p = 0\n",
        "with open('fawiki-20201120-pages-articles-multistream.xml', 'rb') as in_xml:\n",
        "    Tokens = []\n",
        "    articles = []\n",
        "    for record in Wikipedia(in_xml, tag='page'):\n",
        "        # print(norm.normalizer(record['revision'][0]))\n",
        "        try:\n",
        "            article = norm.tokenize(norm.normalizeContent(record['revision'][0]))\n",
        "            article = norm.getFreshTokens(article, pat)\n",
        "            articles.append(article)\n",
        "            Tokens.extend(article)\n",
        "        except Exception as ex:\n",
        "            pass\n",
        "        # break\n",
        "        if p%100000==0:\n",
        "            Tokens = list(set(Tokens))\n",
        "            print(p//100000, len(Tokens), end=' ')\n",
        "            # with open(\"fawiki-articles-normalized.txt\", 'a+') as nf:\n",
        "            #     nf.writelines(l)\n",
        "            # l = []\n",
        "        p += 1\n",
        "Tokens = list(set(Tokens))\n",
        "print(p//100000, len(Tokens), end=' ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 58 1 412644 2 521851 3 580828 4 606206 5 637657 6 683590 7 706455 8 721007 9 724757 10 732584 11 759837 12 767698 13 787518 14 791046 15 805791 16 814794 17 838720 18 862006 19 889675 20 906355 21 925528 22 964099 23 984898 24 1026406 25 1052223 26 1075883 27 1100776 28 1107859 29 1120711 30 1142157 31 1210634 32 1235463 33 1263499 33 1283000 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N3lBKb79T14",
        "outputId": "cc472011-1083-4e7b-c6c4-6fed57af5fe8"
      },
      "source": [
        "# Abstracts\n",
        "import xml.etree.ElementTree as ET\n",
        "from time import time\n",
        "root = ET.parse('fawiki-20201101-abstract.xml').getroot()\n",
        "Tokens = []\n",
        "p = 0\n",
        "for i, item in enumerate(root.findall('./doc')):\n",
        "    if len(item)>2:\n",
        "        try:\n",
        "            article = norm.tokenize(norm.normalizeContent(item[2].text))\n",
        "            article = norm.getFreshTokens(article, pat)\n",
        "            Tokens.extend(article)\n",
        "        except Exception as ex:\n",
        "            pass \n",
        "    if p%100000==0:\n",
        "        Tokens = list(set(Tokens))\n",
        "        print(p//100000, len(Tokens), end=' ')\n",
        "    p += 1\n",
        "    # TODO \n",
        "Tokens = list(set(Tokens))\n",
        "print(p//100000, len(Tokens), end=' ')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0 1 87918 2 126096 3 166273 4 197538 5 244991 6 263097 7 287454 7 303634 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjHawLCyLMuH"
      },
      "source": [
        "articles[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxnMQNk_MA9a",
        "outputId": "36724d50-d06d-491a-8f2e-b3253f30e408"
      },
      "source": [
        "TokensNum = {el:0 for el in Tokens}\n",
        "p = 0\n",
        "for article in articles:\n",
        "    for token in article:\n",
        "        if TokensNum[token]>3:\n",
        "            continue\n",
        "        try:\n",
        "            TokensNum[token] += 1\n",
        "        except:\n",
        "            pass\n",
        "    if p%100000==0:\n",
        "        print(p//100000, end=' ')\n",
        "    p += 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0 1 100000 2 200000 3 300000 4 400000 5 500000 6 600000 7 700000 8 800000 9 900000 10 1000000 11 1100000 12 1200000 13 1300000 14 1400000 15 1500000 16 1600000 17 1700000 18 1800000 19 1900000 20 2000000 21 2100000 22 2200000 23 2300000 24 2400000 25 2500000 26 2600000 27 2700000 28 2800000 29 2900000 30 3000000 31 3100000 32 3200000 33 3300000 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2PMLK_zMrMF"
      },
      "source": [
        "for key, val in zip(list(TokensNum.keys())[:100],list(TokensNum.values())[:100]):\n",
        "    print(key, val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGiiklLFcWsM",
        "outputId": "0db5cf3d-4e5f-43ad-d2b8-e15b4a4ea50c"
      },
      "source": [
        "newTokens = []\n",
        "p = 0\n",
        "for token, num in TokensNum.items():\n",
        "    if num>3:\n",
        "        newTokens.append(token)\n",
        "    if p%100000==0:\n",
        "        print(p//100000, end=' ')\n",
        "    p += 1\n",
        "print('\\n', len(newTokens))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 1 2 3 4 5 6 7 8 9 10 11 12 \n",
            " 396788\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVQDUJF_yWdH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5009f01-8e0e-4a68-9f7b-947a5386533a"
      },
      "source": [
        "newTokens.sort()\n",
        "newTokens[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['__ÿ®€å\\u200cŸÜ⁄Øÿßÿ±ÿÆÿßŸÜŸá__',\n",
              " '__ÿ®€å\\u200cŸæ€åŸàŸÜÿØÿ®ÿÆÿ¥__',\n",
              " '__ÿ™ÿ∫€å€åÿ±ŸÖÿ≥€åÿ±ÿ´ÿßÿ®ÿ™__',\n",
              " '_ÿ¢ÿ®ÿßŸÜ',\n",
              " '_ÿ¢ÿ®ÿ±ÿßŸÖÿ≤',\n",
              " '_ÿ¢ÿ∞ÿ±',\n",
              " '_ÿ¢ÿ±⁄òÿßŸÜÿ™€åŸÜ',\n",
              " '_ÿ¢ÿ±€åÿ≤ŸàŸÜÿß',\n",
              " '_ÿ¢ŸÑÿßÿ®ÿßŸÖÿß',\n",
              " '_ÿ¢ŸÑÿßÿ≥⁄©ÿß']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OcJCVfuugLZk"
      },
      "source": [
        "newTokenss = newTokens.copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m24DWfK5f41H"
      },
      "source": [
        "newTokenss = [str.replace(token, '_', ' ') for token in newTokenss]\n",
        "newTokenss = [str.strip(token) for token in newTokenss]\n",
        "newTokenss = list(set(newTokenss))\n",
        "# newTokenss.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxkxWj1VeCb-"
      },
      "source": [
        "newTokenss.sort()\n",
        "newTokenss[:100]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnb0ZLLpbVL1"
      },
      "source": [
        "with open('Vocabulary_v5.0-36char-afterTokenize&RemoveUnderlineSplitByN.txt', 'w') as f:\n",
        "    f.write('\\n'.join(newTokenss))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vkkb1pDV_DMT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ead717b8-6dd7-467c-e457-e951d5d235d4"
      },
      "source": [
        "newTokens = []\n",
        "for token in newTokenss:\n",
        "    newTokens.append(token)\n",
        "    token = token.split(' ')\n",
        "    if len(token)>1:\n",
        "        for word in token:\n",
        "            if len(word)>1:\n",
        "                newTokens.append(word)\n",
        "        if len(token)==3:\n",
        "            for i in range(len(token)-1):\n",
        "                newTokens.append(str(token[i]+' '+token[i+1]))\n",
        "        if len(token)>3:\n",
        "            for i in range(len(token)-2):\n",
        "                newTokens.append(str(token[i]+' '+token[i+1]+' '+token[i+2]))\n",
        "newTokens = [str.strip(token) for token in newTokens]\n",
        "newTokens = list(set(newTokens))\n",
        "# newTokens.sort()\n",
        "len(newTokens), newTokens[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1341556,\n",
              " ['ÿØ€å⁄©ÿ≥ÿ™',\n",
              "  'ÿ®ÿß⁄©ÿ™ÿ±€åŸàŸÅÿß⁄ò ÿ±ÿ¥ÿ™Ÿá\\u200cÿß€å',\n",
              "  'ÿ±ŸàŸÅŸÅŸàÿ±ÿ™',\n",
              "  'ŸÇÿ®€åÿ≠\\u200cÿßŸÑŸÑŸÇÿß',\n",
              "  'ÿ≠ÿßÿ¥ÿ®€åŸá',\n",
              "  'ÿ™ÿ¥⁄©€å⁄©\\u200c⁄©ÿ±ÿØŸÜ\\u200cŸáÿß€å',\n",
              "  'ŸÜŸÑÿØŸàÿßŸÜ€å',\n",
              "  'ÿß€åŸÖÿ≠Ÿàÿ™ÿ®',\n",
              "  'ÿ™ÿß⁄©ÿß⁄©ÿßÿ≤Ÿà',\n",
              "  'ÿ™ŸÑŸá\\u200cŸÜŸàŸÑÿß'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "olJ6snUDvSho"
      },
      "source": [
        "with open('Vocabulary_v4.0-36char-Tokenize&RemoveUnderline2Words3WordsSplitByN-ReadyToBuildDataSet.txt', 'w') as f:\n",
        "    f.write('\\n'.join(newTokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "458aQdGddKFL",
        "outputId": "3c669913-c94e-4846-dc20-125d72b5c806"
      },
      "source": [
        "len(newTokens)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1341556"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_n1AE3-ctZg"
      },
      "source": [
        "### Draft"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "--0MdnaP3Mpj"
      },
      "source": [
        "!head -n400 fawiki-20201120-pages-articles-multistream.xml"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnYX5b1JgCcE",
        "outputId": "62a04d8e-9991-4e7f-e6e7-8b7608044c0b"
      },
      "source": [
        "chars = []\n",
        "for i, token in enumerate(l):\n",
        "    t = list(token)\n",
        "    for c in t:\n",
        "        if (ord(c)>=1536 and ord(c)<=1610) or (ord(c)>=1646 and ord(c)<=1749) or (ord(c)>=1786 and ord(c)<=1790) or (ord(c)>=1872 and ord(c)<=1957) \\\n",
        "             or (ord(c)>=2208 and ord(c)<=2237) or (ord(c)>=8192 and ord(c)<=8209):\n",
        "            chars.append(c)\n",
        "    if i%100000==0:\n",
        "        chars = list(set(chars))\n",
        "        print(len(chars), end=\" \")\n",
        "chars = list(set(chars))\n",
        "print(len(chars), end=\" \")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 112 136 145 154 159 167 179 180 189 195 196 201 205 210 210 213 214 "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JQ88qGRein1w"
      },
      "source": [
        "for i in range(0,2000):\n",
        "    print(chr(i) , i)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LV8puFboPUFT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}